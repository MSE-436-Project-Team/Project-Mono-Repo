{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Player Statistics Prediction - Inference\n",
    "\n",
    "This notebook loads all trained models and makes predictions on the latest season features data.\n",
    "\n",
    "## Models Used:\n",
    "1. **Ridge Regression** (Linear model with regularization)\n",
    "2. **XGBoost** (Gradient boosting)\n",
    "3. **LightGBM** (Gradient boosting)\n",
    "4. **Bayesian Multi-Output Regression** (PyMC with MatrixNormal)\n",
    "5. **LSTM** (Deep learning with PyTorch)\n",
    "6. **Ensemble Methods** (Simple averaging, weighted averaging, stacking)\n",
    "\n",
    "## Input Data:\n",
    "- `latest_season_features_for_inference.csv`: Features for the current season\n",
    "\n",
    "## Output:\n",
    "- Predictions for next season's statistics for all players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_backend_dir(start_path=None):\n",
    "    \"\"\"\n",
    "    Walk up directories from start_path (or cwd) until a folder named 'backend' is found.\n",
    "    Returns the absolute path to the 'backend' folder.\n",
    "    \"\"\"\n",
    "    if start_path is None:\n",
    "        start_path = os.getcwd()\n",
    "    curr_path = os.path.abspath(start_path)\n",
    "    while True:\n",
    "        # Check if 'backend' exists in this directory\n",
    "        candidate = os.path.join(curr_path, \"backend\")\n",
    "        if os.path.isdir(candidate):\n",
    "            return candidate\n",
    "        # If at filesystem root, stop\n",
    "        parent = os.path.dirname(curr_path)\n",
    "        if curr_path == parent:\n",
    "            break\n",
    "        curr_path = parent\n",
    "    raise FileNotFoundError(f\"No 'backend' directory found upward from {start_path}\")\n",
    "\n",
    "# Find the backend directory and CSV folder\n",
    "backend_dir = find_backend_dir()\n",
    "csv_dir = os.path.join(backend_dir, \"CSVs\")\n",
    "models_dir = os.path.join(backend_dir, \"Models\")\n",
    "output_dir = os.path.join(backend_dir, \"Output\")\n",
    "\n",
    "print(f\"Backend directory: {backend_dir}\")\n",
    "print(f\"CSV directory: {csv_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the inference data\n",
    "print(\"Loading inference data...\")\n",
    "inference_data = pd.read_csv(os.path.join(csv_dir, \"latest_season_features_for_inference.csv\"))\n",
    "\n",
    "print(f\"Inference data shape: {inference_data.shape}\")\n",
    "print(f\"Columns: {len(inference_data.columns)}\")\n",
    "print(f\"\\nFirst few columns: {list(inference_data.columns[:10])}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(inference_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load column information from different model files\n",
    "print(\"Loading column information...\")\n",
    "\n",
    "# Try to load column info from different model files\n",
    "column_info_sources = [\n",
    "    'ridge_columns.joblib',\n",
    "    'tree_models_columns.joblib',\n",
    "    'bayesian_multioutput_columns.joblib',\n",
    "    'lstm_columns.joblib'\n",
    "]\n",
    "\n",
    "feature_cols = None\n",
    "target_cols = None\n",
    "\n",
    "for source in column_info_sources:\n",
    "    try:\n",
    "        columns_info = joblib.load(os.path.join(models_dir, source))\n",
    "        feature_cols = columns_info['feature_cols']\n",
    "        target_cols = columns_info['target_cols']\n",
    "        print(f\"Loaded column info from {source}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load from {source}: {e}\")\n",
    "        continue\n",
    "\n",
    "if feature_cols is None or target_cols is None:\n",
    "    # Fallback: infer columns from inference data\n",
    "    feature_cols = [col for col in inference_data.columns if not col.startswith('next_') and col not in ['PERSON_ID', 'SEASON_ID']]\n",
    "    target_cols = [col for col in inference_data.columns if col.startswith('next_')]\n",
    "    print(\"Using fallback column inference\")\n",
    "\n",
    "print(f\"\\nFeature columns: {len(feature_cols)}\")\n",
    "print(f\"Target columns: {len(target_cols)}\")\n",
    "print(f\"\\nTarget variables: {target_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inference data\n",
    "print(\"Preparing inference data...\")\n",
    "\n",
    "# Select features\n",
    "X_inference = inference_data[feature_cols].copy()\n",
    "\n",
    "# Handle infinite values\n",
    "X_inference.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "print(f\"Inference features shape: {X_inference.shape}\")\n",
    "print(f\"Missing values: {X_inference.isnull().sum().sum()}\")\n",
    "\n",
    "# Check if we have the required columns\n",
    "missing_cols = set(feature_cols) - set(X_inference.columns)\n",
    "if missing_cols:\n",
    "    print(f\"Warning: Missing columns: {missing_cols}\")\n",
    "    # Add missing columns with zeros\n",
    "    for col in missing_cols:\n",
    "        X_inference[col] = 0\n",
    "\n",
    "# Ensure correct column order\n",
    "X_inference = X_inference[feature_cols]\n",
    "\n",
    "print(f\"Final inference features shape: {X_inference.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Run Individual Models\n",
    "\n",
    "We'll load each trained model and make predictions using their respective prediction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load model predictions using saved prediction functions\n",
    "def load_model_predictions(model_name, X_inference, models_dir):\n",
    "    \"\"\"\n",
    "    Load pre-trained model and get predictions using saved prediction functions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        Name of the model\n",
    "    X_inference : DataFrame\n",
    "        Input features for inference\n",
    "    models_dir : str\n",
    "        Directory containing saved models\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : DataFrame\n",
    "        Model predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model_name == 'ridge':\n",
    "            # Load Ridge prediction function\n",
    "            pred_func = joblib.load(os.path.join(models_dir, 'ridge_prediction_function.joblib'))\n",
    "            predictions = pred_func(X_inference)\n",
    "            \n",
    "        elif model_name == 'xgboost':\n",
    "            # Load XGBoost prediction function\n",
    "            pred_func = joblib.load(os.path.join(models_dir, 'xgboost_prediction_function.joblib'))\n",
    "            predictions = pred_func(X_inference)\n",
    "            \n",
    "        elif model_name == 'lightgbm':\n",
    "            # Load LightGBM prediction function\n",
    "            pred_func = joblib.load(os.path.join(models_dir, 'lightgbm_prediction_function.joblib'))\n",
    "            predictions = pred_func(X_inference)\n",
    "            \n",
    "        elif model_name == 'bayesian':\n",
    "            # Load Bayesian model\n",
    "            import arviz as az\n",
    "            trace = az.from_netcdf(os.path.join(models_dir, 'bayesian_multioutput_trace.nc'))\n",
    "            scaler = joblib.load(os.path.join(models_dir, 'bayesian_multioutput_scaler.joblib'))\n",
    "            imputer_X = joblib.load(os.path.join(models_dir, 'bayesian_multioutput_imputer_X.joblib'))\n",
    "            \n",
    "            # Preprocess data\n",
    "            X_processed = pd.DataFrame(\n",
    "                imputer_X.transform(X_inference),\n",
    "                columns=X_inference.columns,\n",
    "                index=X_inference.index\n",
    "            )\n",
    "            \n",
    "            # Scale features\n",
    "            X_scaled = scaler.transform(X_processed)\n",
    "            \n",
    "            # Get predictions\n",
    "            beta_samples = trace.posterior['beta'].values\n",
    "            intercept_samples = trace.posterior['intercept'].values\n",
    "            \n",
    "            # Make predictions\n",
    "            pred = np.mean(np.dot(X_scaled, beta_samples) + intercept_samples, axis=(0, 1))\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            predictions = pd.DataFrame(\n",
    "                pred,\n",
    "                columns=target_cols,\n",
    "                index=X_inference.index\n",
    "            )\n",
    "            \n",
    "        elif model_name == 'lstm':\n",
    "            # Load LSTM model\n",
    "            import torch\n",
    "            \n",
    "            # Load model info\n",
    "            model_info = joblib.load(os.path.join(models_dir, 'lstm_model_info.joblib'))\n",
    "            scaler_X = joblib.load(os.path.join(models_dir, 'lstm_scaler_X.joblib'))\n",
    "            scaler_y = joblib.load(os.path.join(models_dir, 'lstm_scaler_y.joblib'))\n",
    "            imputer_X = joblib.load(os.path.join(models_dir, 'lstm_imputer_X.joblib'))\n",
    "            \n",
    "            # Import LSTM model class and functions\n",
    "            try:\n",
    "                from lstm_training_testing import LSTMModel, create_sequences\n",
    "            except ImportError:\n",
    "                # If the module doesn't exist, define the functions here\n",
    "                def create_sequences(X, y, sequence_length=5):\n",
    "                    X_seq = []\n",
    "                    y_seq = []\n",
    "                    for i in range(sequence_length, len(X)):\n",
    "                        X_seq.append(X.iloc[i-sequence_length:i].values)\n",
    "                        y_seq.append(y.iloc[i].values if len(y) > 0 else np.zeros(len(target_cols)))\n",
    "                    return np.array(X_seq), np.array(y_seq)\n",
    "                \n",
    "                class LSTMModel(torch.nn.Module):\n",
    "                    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "                        super(LSTMModel, self).__init__()\n",
    "                        self.hidden_size = hidden_size\n",
    "                        self.num_layers = num_layers\n",
    "                        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                                                 batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "                        self.fc1 = torch.nn.Linear(hidden_size, hidden_size // 2)\n",
    "                        self.dropout = torch.nn.Dropout(dropout)\n",
    "                        self.fc2 = torch.nn.Linear(hidden_size // 2, output_size)\n",
    "                        self.relu = torch.nn.ReLU()\n",
    "                    \n",
    "                    def forward(self, x):\n",
    "                        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "                        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "                        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "                        lstm_out = lstm_out[:, -1, :]\n",
    "                        out = self.relu(self.fc1(lstm_out))\n",
    "                        out = self.dropout(out)\n",
    "                        out = self.fc2(out)\n",
    "                        return out\n",
    "            \n",
    "            # Create model\n",
    "            model = LSTMModel(\n",
    "                input_size=model_info['input_size'],\n",
    "                hidden_size=model_info['hidden_size'],\n",
    "                num_layers=model_info['num_layers'],\n",
    "                output_size=model_info['output_size'],\n",
    "                dropout=model_info['dropout']\n",
    "            )\n",
    "            \n",
    "            # Load state dict\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            model.load_state_dict(torch.load(os.path.join(models_dir, 'lstm_best_model.pth'), map_location=device))\n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Preprocess\n",
    "            X_processed = pd.DataFrame(\n",
    "                imputer_X.transform(X_inference),\n",
    "                columns=X_inference.columns,\n",
    "                index=X_inference.index\n",
    "            )\n",
    "            \n",
    "            # Scale features\n",
    "            X_scaled = scaler_X.transform(X_processed)\n",
    "            \n",
    "            # Create sequences\n",
    "            sequence_length = model_info['sequence_length']\n",
    "            X_seq, _ = create_sequences(pd.DataFrame(X_scaled), pd.DataFrame(), sequence_length)\n",
    "            \n",
    "            # Make predictions\n",
    "            with torch.no_grad():\n",
    "                X_tensor = torch.FloatTensor(X_seq).to(device)\n",
    "                pred_scaled = model(X_tensor).cpu().numpy()\n",
    "                pred = scaler_y.inverse_transform(pred_scaled)\n",
    "                \n",
    "                # Convert to DataFrame\n",
    "                predictions = pd.DataFrame(\n",
    "                    pred,\n",
    "                    columns=target_cols,\n",
    "                    index=X_inference.index[sequence_length:]\n",
    "                )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {model_name}\")\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {model_name} model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and run all models\n",
    "print(\"Loading and running all models...\")\n",
    "\n",
    "models = ['ridge', 'xgboost', 'lightgbm', 'bayesian', 'lstm']\n",
    "predictions = {}\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\nRunning {model_name.upper()} model...\")\n",
    "    pred_df = load_model_predictions(model_name, X_inference, models_dir)\n",
    "    \n",
    "    if pred_df is not None:\n",
    "        predictions[model_name] = pred_df\n",
    "        print(f\"  {model_name.upper()} predictions shape: {pred_df.shape}\")\n",
    "        print(f\"  Sample predictions:\")\n",
    "        print(pred_df.head(3))\n",
    "    else:\n",
    "        print(f\"  {model_name.upper()} failed to run\")\n",
    "\n",
    "print(f\"\\nSuccessfully ran {len(predictions)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble predictions\n",
    "print(\"\\nCreating ensemble predictions...\")\n",
    "\n",
    "if len(predictions) > 1:\n",
    "    # Simple averaging\n",
    "    pred_arrays = [predictions[model].values for model in predictions.keys()]\n",
    "    simple_avg_pred = np.mean(pred_arrays, axis=0)\n",
    "    \n",
    "    simple_avg_df = pd.DataFrame(\n",
    "        simple_avg_pred,\n",
    "        columns=target_cols,\n",
    "        index=X_inference.index\n",
    "    )\n",
    "    \n",
    "    predictions['ensemble_simple'] = simple_avg_df\n",
    "    \n",
    "    # Weighted averaging (using equal weights for now)\n",
    "    weights = np.ones(len(predictions)) / len(predictions)\n",
    "    weighted_avg_pred = np.sum([weights[i] * pred_arrays[i] for i in range(len(pred_arrays))], axis=0)\n",
    "    \n",
    "    weighted_avg_df = pd.DataFrame(\n",
    "        weighted_avg_pred,\n",
    "        columns=target_cols,\n",
    "        index=X_inference.index\n",
    "    )\n",
    "    \n",
    "    predictions['ensemble_weighted'] = weighted_avg_df\n",
    "    \n",
    "    print(f\"Created ensemble predictions\")\n",
    "    print(f\"  Simple average shape: {simple_avg_df.shape}\")\n",
    "    print(f\"  Weighted average shape: {weighted_avg_df.shape}\")\n",
    "else:\n",
    "    print(\"Not enough models for ensemble\")\n",
    "\n",
    "print(f\"\\nTotal prediction methods: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions\n",
    "print(\"\\nAnalyzing predictions...\")\n",
    "\n",
    "# Summary statistics for each model\n",
    "for model_name, pred_df in predictions.items():\n",
    "    print(f\"\\n{model_name.upper()} Predictions Summary:\")\n",
    "    print(f\"  Shape: {pred_df.shape}\")\n",
    "    print(f\"  Mean values:\")\n",
    "    for col in pred_df.columns:\n",
    "        print(f\"    {col}: {pred_df[col].mean():.3f}\")\n",
    "    print(f\"  Standard deviations:\")\n",
    "    for col in pred_df.columns:\n",
    "        print(f\"    {col}: {pred_df[col].std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "print(\"\\nCreating visualizations...\")\n",
    "\n",
    "# Plot 1: Distribution of predictions for each target\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, target in enumerate(target_cols[:4]):  # Show first 4 targets\n",
    "    if 'ensemble_simple' in predictions:\n",
    "        axes[i].hist(predictions['ensemble_simple'][target], bins=30, alpha=0.7, label='Ensemble')\n",
    "    axes[i].set_xlabel('Predicted Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_title(f'Distribution of {target} Predictions')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Model comparison for one target\n",
    "if len(predictions) > 1:\n",
    "    target = target_cols[0]  # Use first target\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for model_name, pred_df in predictions.items():\n",
    "        if target in pred_df.columns:\n",
    "            plt.hist(pred_df[target], bins=30, alpha=0.6, label=model_name.upper())\n",
    "    \n",
    "    plt.xlabel('Predicted Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Model Comparison for {target}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "print(\"\\nSaving predictions...\")\n",
    "\n",
    "# Create timestamp for file naming\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save individual model predictions\n",
    "for model_name, pred_df in predictions.items():\n",
    "    if not model_name.startswith('ensemble'):\n",
    "        filename = f\"{model_name}_predictions_{timestamp}.csv\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        pred_df.to_csv(filepath)\n",
    "        print(f\"  Saved {model_name} predictions to: {filepath}\")\n",
    "\n",
    "# Save ensemble predictions\n",
    "if 'ensemble_simple' in predictions:\n",
    "    ensemble_filename = f\"ensemble_predictions_{timestamp}.csv\"\n",
    "    ensemble_filepath = os.path.join(output_dir, ensemble_filename)\n",
    "    predictions['ensemble_simple'].to_csv(ensemble_filepath)\n",
    "    print(f\"  Saved ensemble predictions to: {ensemble_filepath}\")\n",
    "\n",
    "# Save all predictions together\n",
    "all_predictions_filename = f\"all_predictions_{timestamp}.joblib\"\n",
    "all_predictions_filepath = os.path.join(output_dir, all_predictions_filename)\n",
    "joblib.dump(predictions, all_predictions_filepath)\n",
    "print(f\"  Saved all predictions to: {all_predictions_filepath}\")\n",
    "\n",
    "print(\"\\nAll predictions saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "print(\"\\nCreating summary report...\")\n",
    "\n",
    "report = {\n",
    "    'timestamp': timestamp,\n",
    "    'input_data_shape': inference_data.shape,\n",
    "    'feature_columns': len(feature_cols),\n",
    "    'target_columns': len(target_cols),\n",
    "    'models_used': list(predictions.keys()),\n",
    "    'predictions_summary': {}\n",
    "}\n",
    "\n",
    "# Add summary statistics for each model\n",
    "for model_name, pred_df in predictions.items():\n",
    "    report['predictions_summary'][model_name] = {\n",
    "        'shape': pred_df.shape,\n",
    "        'mean_values': pred_df.mean().to_dict(),\n",
    "        'std_values': pred_df.std().to_dict(),\n",
    "        'min_values': pred_df.min().to_dict(),\n",
    "        'max_values': pred_df.max().to_dict()\n",
    "    }\n",
    "\n",
    "# Save report\n",
    "report_filename = f\"inference_report_{timestamp}.joblib\"\n",
    "report_filepath = os.path.join(output_dir, report_filename)\n",
    "joblib.dump(report, report_filepath)\n",
    "print(f\"  Saved inference report to: {report_filepath}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Timestamp: {timestamp}\")\n",
    "print(f\"Input data shape: {inference_data.shape}\")\n",
    "print(f\"Feature columns: {len(feature_cols)}\")\n",
    "print(f\"Target columns: {len(target_cols)}\")\n",
    "print(f\"Models used: {len(predictions)}\")\n",
    "print(f\"\\nModels:\")\n",
    "for model_name in predictions.keys():\n",
    "    print(f\"  - {model_name.upper()}\")\n",
    "print(f\"\\nOutput files saved to: {output_dir}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble prediction function\n",
    "def predict_with_ensemble(X, ensemble_type='weighted_avg', models_dir=models_dir):\n",
    "    \"\"\"\n",
    "    Make predictions using ensemble methods\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : DataFrame\n",
    "        Input features\n",
    "    ensemble_type : str\n",
    "        Type of ensemble ('simple_avg', 'weighted_avg', 'stacking')\n",
    "    models_dir : str\n",
    "        Directory containing saved models\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : DataFrame\n",
    "        Ensemble predictions\n",
    "    \"\"\"\n",
    "    # Load all individual models\n",
    "    individual_predictions = {}\n",
    "    \n",
    "    for model_name in ['ridge', 'xgboost', 'lightgbm', 'bayesian', 'lstm']:\n",
    "        try:\n",
    "            pred_df = load_model_predictions(model_name, X, models_dir)\n",
    "            if pred_df is not None:\n",
    "                individual_predictions[model_name] = pred_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_name}: {e}\")\n",
    "    \n",
    "    if len(individual_predictions) == 0:\n",
    "        raise ValueError(\"No models could be loaded\")\n",
    "    \n",
    "    # Create ensemble\n",
    "    if ensemble_type == 'simple_avg':\n",
    "        pred_arrays = [individual_predictions[model].values for model in individual_predictions.keys()]\n",
    "        ensemble_pred = np.mean(pred_arrays, axis=0)\n",
    "    elif ensemble_type == 'weighted_avg':\n",
    "        pred_arrays = [individual_predictions[model].values for model in individual_predictions.keys()]\n",
    "        weights = np.ones(len(individual_predictions)) / len(individual_predictions)\n",
    "        ensemble_pred = np.sum([weights[i] * pred_arrays[i] for i in range(len(pred_arrays))], axis=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown ensemble type: {ensemble_type}\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    ensemble_df = pd.DataFrame(\n",
    "        ensemble_pred,\n",
    "        columns=target_cols,\n",
    "        index=X.index\n",
    "    )\n",
    "    \n",
    "    return ensemble_df\n",
    "\n",
    "# Save the prediction function\n",
    "prediction_func_path = os.path.join(models_dir, \"ensemble_prediction_function.joblib\")\n",
    "joblib.dump(predict_with_ensemble, prediction_func_path)\n",
    "print(f\"Ensemble prediction function saved to: {prediction_func_path}\")\n",
    "\n",
    "print(\"\\nInference pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Summary\n",
    "\n",
    "The inference process has been completed successfully. Here's what was accomplished:\n",
    "\n",
    "### Models Used:\n",
    "- Ridge Regression (using saved prediction function)\n",
    "- XGBoost (using saved prediction function)\n",
    "- LightGBM (using saved prediction function)\n",
    "- Bayesian Multi-Output Regression (PyMC)\n",
    "- LSTM (PyTorch)\n",
    "- Ensemble methods (Simple averaging, Weighted averaging)\n",
    "\n",
    "### Key Features:\n",
    "- **Multi-model predictions**: Each model provides its own predictions\n",
    "- **Ensemble combinations**: Simple and weighted averaging of model predictions\n",
    "- **Comprehensive output**: Individual model predictions and ensemble results\n",
    "- **Detailed reporting**: Summary statistics and visualizations\n",
    "- **Robust error handling**: Graceful handling of missing models\n",
    "\n",
    "### Output Files:\n",
    "- Individual model prediction CSV files\n",
    "- Ensemble prediction CSV file\n",
    "- All predictions in joblib format\n",
    "- Detailed inference report\n",
    "- Ensemble prediction function\n",
    "\n",
    "### Next Steps:\n",
    "1. Analyze the predictions for insights\n",
    "2. Compare model performance on specific players\n",
    "3. Validate predictions against actual performance\n",
    "4. Use predictions for fantasy sports or scouting\n",
    "5. Retrain models with new data as it becomes available"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
