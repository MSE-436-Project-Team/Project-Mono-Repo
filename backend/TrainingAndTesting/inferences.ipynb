{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Player Statistics Prediction - Inference\n",
    "\n",
    "This notebook loads all trained models and makes predictions on the latest season features data for the 2025-26 season.\n",
    "\n",
    "## Models Used:\n",
    "1. **Ridge Regression** (Linear model with regularization)\n",
    "2. **XGBoost** (Gradient boosting)\n",
    "3. **LightGBM** (Gradient boosting)\n",
    "4. **Bayesian Multi-Output Regression** (PyMC with MatrixNormal)\n",
    "5. **LSTM** (Deep learning with PyTorch)\n",
    "6. **Transformer** (Deep learning with PyTorch)\n",
    "7. **Ensemble Methods** (Simple averaging, Weighted averaging, Stacking)\n",
    "\n",
    "## Input Data:\n",
    "- `latest_season_features_for_inference.csv`: Features for the 2024-25 season\n",
    "\n",
    "## Output:\n",
    "- Individual model predictions saved as CSV files in Output folder with player mapping\n",
    "- Ensemble predictions saved as CSV files in Output folder with player mapping\n",
    "- Comprehensive comparison and analysis\n",
    "- Predictions for 2025-26 season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backend directory: /Users/jeevanparmar/Uni/MSE 436/Project-Mono-Repo/backend\n",
      "CSV directory: /Users/jeevanparmar/Uni/MSE 436/Project-Mono-Repo/backend/CSVs\n",
      "Models directory: /Users/jeevanparmar/Uni/MSE 436/Project-Mono-Repo/backend/Models\n",
      "Output directory: /Users/jeevanparmar/Uni/MSE 436/Project-Mono-Repo/backend/Output\n"
     ]
    }
   ],
   "source": [
    "def find_backend_dir(start_path=None):\n",
    "    \"\"\"\n",
    "    Walk up directories from start_path (or cwd) until a folder named 'backend' is found.\n",
    "    Returns the absolute path to the 'backend' folder.\n",
    "    \"\"\"\n",
    "    if start_path is None:\n",
    "        start_path = os.getcwd()\n",
    "    curr_path = os.path.abspath(start_path)\n",
    "    while True:\n",
    "        # Check if 'backend' exists in this directory\n",
    "        candidate = os.path.join(curr_path, \"backend\")\n",
    "        if os.path.isdir(candidate):\n",
    "            return candidate\n",
    "        # If at filesystem root, stop\n",
    "        parent = os.path.dirname(curr_path)\n",
    "        if curr_path == parent:\n",
    "            break\n",
    "        curr_path = parent\n",
    "    raise FileNotFoundError(f\"No 'backend' directory found upward from {start_path}\")\n",
    "\n",
    "# Find the backend directory and CSV folder\n",
    "backend_dir = find_backend_dir()\n",
    "csv_dir = os.path.join(backend_dir, \"CSVs\")\n",
    "models_dir = os.path.join(backend_dir, \"Models\")\n",
    "output_dir = os.path.join(backend_dir, \"Output\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Backend directory: {backend_dir}\")\n",
    "print(f\"CSV directory: {csv_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading inference data...\n",
      "Inference data shape: (566, 58)\n",
      "Columns: 58\n",
      "\n",
      "First few columns: ['PERSON_ID', 'SEASON_ID', 'Points', 'Minutes', 'FGM', 'FGA', 'FG%', '3PM', '3PA', '3P%']\n",
      "\n",
      "Sample data:\n",
      "   PERSON_ID SEASON_ID     Points    Minutes       FGM        FGA        FG%  \\\n",
      "0       2544   2024-25  24.428571  34.957143  9.300000  18.142857  51.094286   \n",
      "1     101108   2024-25   8.817073  27.939024  3.036585   7.109756  43.278049   \n",
      "2     200768   2024-25   3.942857  18.885714  1.171429   3.342857  28.868750   \n",
      "3     200782   2024-25   3.000000  19.333333  1.000000   2.333333  45.000000   \n",
      "4     201142   2024-25  26.564516  36.580645  9.548387  18.129032  53.504839   \n",
      "\n",
      "        3PM       3PA        3P%  ...  SEASON_Spring        AGE  \\\n",
      "0  2.128571  5.657143  34.864286  ...           True  40.517454   \n",
      "1  1.707317  4.524390  38.669512  ...           True  40.169747   \n",
      "2  0.828571  2.514286  27.796774  ...           True  39.285421   \n",
      "3  1.000000  2.000000  50.000000  ...           True  40.172485   \n",
      "4  2.580645  6.000000  46.579032  ...           True  36.769336   \n",
      "\n",
      "   EXPERIENCE_YEARS  HEIGHT_INCHES  WEIGHT        BMI  DRAFT_POSITION  \\\n",
      "0                22             81   250.0  26.789705            31.0   \n",
      "1                20             72   175.0  23.734004            34.0   \n",
      "2                19             72   196.0  26.582084            54.0   \n",
      "3                19             77   245.0  29.052438            95.0   \n",
      "4                18             83   240.0  24.493622            32.0   \n",
      "\n",
      "   TOP_10_PICK  LOTTERY_PICK  POSITION_CATEGORY  \n",
      "0            0             0                  3  \n",
      "1            0             0                  1  \n",
      "2            0             0                  1  \n",
      "3            0             0                  3  \n",
      "4            0             0                  3  \n",
      "\n",
      "[5 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the inference data\n",
    "print(\"Loading inference data...\")\n",
    "inference_data = pd.read_csv(os.path.join(csv_dir, \"latest_season_features_for_inference.csv\"))\n",
    "\n",
    "print(f\"Inference data shape: {inference_data.shape}\")\n",
    "print(f\"Columns: {len(inference_data.columns)}\")\n",
    "print(f\"\\nFirst few columns: {list(inference_data.columns[:10])}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(inference_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading column information...\n",
      "Loaded column info from ridge_columns.joblib\n",
      "\n",
      "Feature columns: 56\n",
      "Target columns: 21\n",
      "\n",
      "Target variables: ['next_Points', 'next_FTM', 'next_FTA', 'next_FGM', 'next_FGA', 'next_TO', 'next_STL', 'next_BLK', 'next_PF', 'next_USAGE_RATE', 'next_OREB', 'next_DREB', 'next_AST', 'next_REB', 'next_Minutes', 'next_3PM', 'next_3PA', 'next_3P%', 'next_FT%', 'next_FG%', 'next_GAME_EFFICIENCY']\n"
     ]
    }
   ],
   "source": [
    "# Load column information from different model files\n",
    "print(\"Loading column information...\")\n",
    "\n",
    "# Try to load column info from different model files\n",
    "column_info_sources = [\n",
    "    'ridge_columns.joblib',\n",
    "    'tree_models_columns.joblib',\n",
    "    'bayesian_multioutput_columns.joblib',\n",
    "    'lstm_columns.joblib',\n",
    "    'transformer_columns.joblib'\n",
    "]\n",
    "\n",
    "feature_cols = None\n",
    "target_cols = None\n",
    "\n",
    "for source in column_info_sources:\n",
    "    try:\n",
    "        columns_info = joblib.load(os.path.join(models_dir, source))\n",
    "        feature_cols = columns_info['feature_cols']\n",
    "        target_cols = columns_info['target_cols']\n",
    "        print(f\"Loaded column info from {source}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load from {source}: {e}\")\n",
    "        continue\n",
    "\n",
    "if feature_cols is None or target_cols is None:\n",
    "    # Fallback: infer columns from inference data\n",
    "    feature_cols = [col for col in inference_data.columns if not col.startswith('next_') and col not in ['PERSON_ID', 'SEASON_ID']]\n",
    "    target_cols = [col for col in inference_data.columns if col.startswith('next_')]\n",
    "    print(\"Using fallback column inference\")\n",
    "\n",
    "print(f\"\\nFeature columns: {len(feature_cols)}\")\n",
    "print(f\"Target columns: {len(target_cols)}\")\n",
    "print(f\"\\nTarget variables: {target_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing inference data...\n",
      "Inference features shape: (566, 56)\n",
      "Missing values: 209\n",
      "Final inference features shape: (566, 56)\n"
     ]
    }
   ],
   "source": [
    "# Prepare inference data\n",
    "print(\"Preparing inference data...\")\n",
    "\n",
    "# Select features\n",
    "X_inference = inference_data[feature_cols].copy()\n",
    "\n",
    "# Handle infinite values\n",
    "X_inference.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "print(f\"Inference features shape: {X_inference.shape}\")\n",
    "print(f\"Missing values: {X_inference.isnull().sum().sum()}\")\n",
    "\n",
    "# Check if we have the required columns\n",
    "missing_cols = set(feature_cols) - set(X_inference.columns)\n",
    "if missing_cols:\n",
    "    print(f\"Warning: Missing columns: {missing_cols}\")\n",
    "    # Add missing columns with zeros\n",
    "    for col in missing_cols:\n",
    "        X_inference[col] = 0\n",
    "\n",
    "# Ensure correct column order\n",
    "X_inference = X_inference[feature_cols]\n",
    "\n",
    "print(f\"Final inference features shape: {X_inference.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Run Individual Models\n",
    "\n",
    "We'll load each trained model and make predictions using their respective prediction functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load model predictions using direct model loading\n",
    "def load_model_predictions(model_name, X_inference, models_dir):\n",
    "    \"\"\"\n",
    "    Load pre-trained model and get predictions by loading models directly\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model_name == 'ridge':\n",
    "            # Load Ridge model directly\n",
    "            model = joblib.load(os.path.join(models_dir, 'ridge_regression_season_model.joblib'))\n",
    "            \n",
    "            # Handle missing values for Ridge\n",
    "            from sklearn.impute import SimpleImputer\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            X_imputed = pd.DataFrame(\n",
    "                imputer.fit_transform(X_inference),\n",
    "                columns=X_inference.columns,\n",
    "                index=X_inference.index\n",
    "            )\n",
    "            \n",
    "            predictions = model.predict(X_imputed)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            predictions_df = pd.DataFrame(\n",
    "                predictions,\n",
    "                columns=target_cols,\n",
    "                index=X_inference.index\n",
    "            )\n",
    "            \n",
    "        elif model_name == 'xgboost':\n",
    "            # Load XGBoost model directly\n",
    "            model = joblib.load(os.path.join(models_dir, 'xgboost_multioutput_tuned_model.joblib'))\n",
    "            predictions = model.predict(X_inference)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            predictions_df = pd.DataFrame(\n",
    "                predictions,\n",
    "                columns=target_cols,\n",
    "                index=X_inference.index\n",
    "            )\n",
    "            \n",
    "        elif model_name == 'lightgbm':\n",
    "            # Load LightGBM model directly\n",
    "            model = joblib.load(os.path.join(models_dir, 'lightgbm_multioutput_tuned_model.joblib'))\n",
    "            predictions = model.predict(X_inference)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            predictions_df = pd.DataFrame(\n",
    "                predictions,\n",
    "                columns=target_cols,\n",
    "                index=X_inference.index\n",
    "            )\n",
    "            \n",
    "        elif model_name == 'bayesian':\n",
    "            # Load Bayesian model\n",
    "            import arviz as az\n",
    "            trace = az.from_netcdf(os.path.join(models_dir, 'bayesian_multioutput_trace.nc'))\n",
    "            scaler = joblib.load(os.path.join(models_dir, 'bayesian_multioutput_scaler.joblib'))\n",
    "            imputer_X = joblib.load(os.path.join(models_dir, 'bayesian_multioutput_imputer_X.joblib'))\n",
    "            \n",
    "            # Preprocess data\n",
    "            X_processed = pd.DataFrame(\n",
    "                imputer_X.transform(X_inference),\n",
    "                columns=X_inference.columns,\n",
    "                index=X_inference.index\n",
    "            )\n",
    "            \n",
    "            # Scale features\n",
    "            X_scaled = scaler.transform(X_processed)\n",
    "            \n",
    "            # Get predictions\n",
    "            beta_samples = trace.posterior['beta'].values\n",
    "            intercept_samples = trace.posterior['intercept'].values\n",
    "            \n",
    "            # Make predictions\n",
    "            pred = np.mean(np.dot(X_scaled, beta_samples) + intercept_samples, axis=(0, 1))\n",
    "            \n",
    "            # Handle shape mismatch - take mean if too many samples\n",
    "            if pred.shape[0] > len(X_inference):\n",
    "                pred_mean = np.mean(pred, axis=0)\n",
    "                pred = np.tile(pred_mean, (len(X_inference), 1))\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            predictions_df = pd.DataFrame(\n",
    "                pred,\n",
    "                columns=target_cols,\n",
    "                index=X_inference.index\n",
    "            )\n",
    "            \n",
    "        elif model_name == 'lstm':\n",
    "            \n",
    "            # Load model components\n",
    "            model_info = joblib.load(os.path.join(models_dir, 'lstm_model_info.joblib'))\n",
    "            scaler_X = joblib.load(os.path.join(models_dir, 'lstm_scaler_X.joblib'))\n",
    "            scaler_y = joblib.load(os.path.join(models_dir, 'lstm_scaler_y.joblib'))\n",
    "            imputer_X = joblib.load(os.path.join(models_dir, 'lstm_imputer_X.joblib'))\n",
    "            imputer_y = joblib.load(os.path.join(models_dir, 'lstm_imputer_y.joblib'))\n",
    "            \n",
    "            # Load the actual trained model\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            # Define LSTM model class (EXACTLY as in training)\n",
    "            class LSTMModel(torch.nn.Module):\n",
    "                def __init__(\n",
    "                    self, input_size, hidden_size, num_layers, output_size,\n",
    "                    dropout=0.5, bidirectional=True\n",
    "                ):\n",
    "                    super(LSTMModel, self).__init__()\n",
    "                    self.hidden_size = hidden_size\n",
    "                    self.num_layers = num_layers\n",
    "                    self.bidirectional = bidirectional\n",
    "                    self.num_directions = 2 if bidirectional else 1\n",
    "\n",
    "                    # LSTM layer\n",
    "                    self.lstm = torch.nn.LSTM(\n",
    "                        input_size=input_size,\n",
    "                        hidden_size=hidden_size,\n",
    "                        num_layers=num_layers,\n",
    "                        batch_first=True,\n",
    "                        dropout=dropout if num_layers > 1 else 0,\n",
    "                        bidirectional=bidirectional\n",
    "                    )\n",
    "\n",
    "                    # Fully connected layers\n",
    "                    self.fc1 = torch.nn.Linear(hidden_size * self.num_directions, hidden_size)\n",
    "                    self.dropout = torch.nn.Dropout(dropout)\n",
    "                    self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "                    self.relu = torch.nn.ReLU()\n",
    "\n",
    "                def forward(self, x):\n",
    "                    batch_size = x.size(0)\n",
    "                    h0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size, device=x.device)\n",
    "                    c0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_size, device=x.device)\n",
    "\n",
    "                    # LSTM forward\n",
    "                    lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "                    # Take the last time step\n",
    "                    lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "                    # Fully connected layers\n",
    "                    out = self.relu(self.fc1(lstm_out))\n",
    "                    out = self.dropout(out)\n",
    "                    out = self.fc2(out)\n",
    "                    return out\n",
    "            \n",
    "            # Define sequence creation function (same as training)\n",
    "            def create_sequences(X, y, sequence_length=10):\n",
    "                X_seq = []\n",
    "                y_seq = []\n",
    "                for i in range(sequence_length, len(X)):\n",
    "                    X_seq.append(X.iloc[i-sequence_length:i].values)\n",
    "                    y_seq.append(y.iloc[i].values)\n",
    "                return np.array(X_seq), np.array(y_seq)\n",
    "            \n",
    "            # Create and load the model with EXACT same parameters as training\n",
    "            model = LSTMModel(\n",
    "                input_size=model_info['input_size'],\n",
    "                hidden_size=model_info['hidden_size'],\n",
    "                num_layers=model_info['num_layers'],\n",
    "                output_size=model_info['output_size'],\n",
    "                dropout=model_info['dropout'],\n",
    "                bidirectional=True  # This was the key missing parameter!\n",
    "            )\n",
    "            \n",
    "            model.load_state_dict(torch.load(os.path.join(models_dir, 'lstm_best_model.pth'), map_location=device))\n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Optimized prediction function\n",
    "            def predict_with_lstm_model_optimized(X, model, scaler_X, scaler_y, imputer_X, imputer_y, \n",
    "                                                feature_cols, target_cols, sequence_length=10):\n",
    "                \"\"\"\n",
    "                Optimized LSTM prediction function\n",
    "                \"\"\"\n",
    "                # Ensure we have the right columns\n",
    "                X = X[feature_cols].copy()\n",
    "                \n",
    "                # Handle missing values\n",
    "                X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "                X_imputed = pd.DataFrame(\n",
    "                    imputer_X.transform(X),\n",
    "                    columns=X.columns,\n",
    "                    index=X.index\n",
    "                )\n",
    "                \n",
    "                # Scale features\n",
    "                X_scaled = pd.DataFrame(\n",
    "                    scaler_X.transform(X_imputed),\n",
    "                    columns=X_imputed.columns,\n",
    "                    index=X_imputed.index\n",
    "                )\n",
    "                \n",
    "                # Create sequences more efficiently\n",
    "                X_seq = []\n",
    "                for i in range(sequence_length, len(X_scaled)):\n",
    "                    X_seq.append(X_scaled.iloc[i-sequence_length:i].values)\n",
    "                X_seq = np.array(X_seq)\n",
    "                \n",
    "                # Convert to tensor\n",
    "                X_tensor = torch.FloatTensor(X_seq).to(device)\n",
    "                \n",
    "                # Make predictions in batches to avoid memory issues\n",
    "                batch_size = 32\n",
    "                pred_scaled_list = []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for i in range(0, len(X_tensor), batch_size):\n",
    "                        batch = X_tensor[i:i+batch_size]\n",
    "                        pred_batch = model(batch).cpu().numpy()\n",
    "                        pred_scaled_list.append(pred_batch)\n",
    "                \n",
    "                pred_scaled = np.concatenate(pred_scaled_list, axis=0)\n",
    "                \n",
    "                # Inverse transform predictions\n",
    "                pred = scaler_y.inverse_transform(pred_scaled)\n",
    "                \n",
    "                # Convert to DataFrame\n",
    "                pred_df = pd.DataFrame(\n",
    "                    pred,\n",
    "                    columns=target_cols,\n",
    "                    index=X.index[sequence_length:]\n",
    "                )\n",
    "                \n",
    "                return pred_df\n",
    "            \n",
    "            # Use the optimized prediction function\n",
    "            predictions_df = predict_with_lstm_model_optimized(\n",
    "                X_inference, model, scaler_X, scaler_y, imputer_X, imputer_y,\n",
    "                feature_cols, target_cols, model_info['sequence_length']\n",
    "            )\n",
    "                \n",
    "        elif model_name == 'transformer':\n",
    "            \n",
    "            # Load model components\n",
    "            scaler = joblib.load(os.path.join(models_dir, 'transformer_scaler.joblib'))\n",
    "            imputer_X = joblib.load(os.path.join(models_dir, 'transformer_imputer_X.joblib'))\n",
    "            columns_info = joblib.load(os.path.join(models_dir, 'transformer_columns.joblib'))\n",
    "            feature_cols_transformer = columns_info['feature_cols']\n",
    "            target_cols_transformer = columns_info['target_cols']\n",
    "            \n",
    "            # Preprocess data\n",
    "            X_processed = pd.DataFrame(\n",
    "                imputer_X.transform(X_inference),\n",
    "                columns=X_inference.columns,\n",
    "                index=X_inference.index\n",
    "            )\n",
    "            \n",
    "            # Scale features\n",
    "            X_scaled = scaler.transform(X_processed)\n",
    "            \n",
    "            # Define Transformer model class (EXACTLY as in training)\n",
    "            class PositionalEncoding(torch.nn.Module):\n",
    "                def __init__(self, d_model, max_len=5000):\n",
    "                    super().__init__()\n",
    "                    pe = torch.zeros(max_len, d_model)\n",
    "                    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "                    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "                    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "                    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "                    pe = pe.unsqueeze(0)\n",
    "                    self.register_buffer('pe', pe)\n",
    "                \n",
    "                def forward(self, x):\n",
    "                    x = x + self.pe[:, :x.size(1), :]\n",
    "                    return x\n",
    "\n",
    "            class TransformerRegressor(torch.nn.Module):\n",
    "                def __init__(self, input_dim, output_dim, d_model=128, nhead=8, num_layers=4, \n",
    "                            dim_feedforward=256, dropout=0.2, seq_len=10):\n",
    "                    super().__init__()\n",
    "                    self.input_linear = torch.nn.Linear(input_dim, d_model)  # Note: input_linear, not input_projection\n",
    "                    self.pos_encoder = PositionalEncoding(d_model, max_len=seq_len)\n",
    "                    encoder_layer = torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "                    self.transformer_encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "                    self.dropout = torch.nn.Dropout(dropout)\n",
    "                    self.fc = torch.nn.Linear(d_model, output_dim)  # Note: fc, not output_projection\n",
    "                \n",
    "                def forward(self, x):\n",
    "                    # x: (batch, seq_len, input_dim)\n",
    "                    x = self.input_linear(x)\n",
    "                    x = self.pos_encoder(x)\n",
    "                    x = self.transformer_encoder(x)\n",
    "                    x = x[:, -1, :]  # Use last time step\n",
    "                    x = self.dropout(x)\n",
    "                    x = self.fc(x)\n",
    "                    return x\n",
    "            \n",
    "            # Optimized prediction function\n",
    "            def predict_with_transformer_model_optimized(X, models_dir, sequence_length=10):\n",
    "                \"\"\"\n",
    "                Optimized Transformer prediction function\n",
    "                \"\"\"\n",
    "                device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                \n",
    "                # Model hyperparameters\n",
    "                d_model = 128\n",
    "                nhead = 8\n",
    "                num_layers = 4\n",
    "                dim_feedforward = 256\n",
    "                dropout = 0.2\n",
    "                \n",
    "                # Create sequences more efficiently\n",
    "                X_seq = []\n",
    "                for i in range(sequence_length, len(X)):\n",
    "                    X_seq.append(X[i-sequence_length:i])\n",
    "                X_seq = np.array(X_seq)\n",
    "                X_tensor = torch.tensor(X_seq, dtype=torch.float32).to(device)\n",
    "                \n",
    "                # Load model and weights (only once)\n",
    "                model = TransformerRegressor(\n",
    "                    input_dim=len(feature_cols_transformer),\n",
    "                    output_dim=len(target_cols_transformer),\n",
    "                    d_model=d_model,\n",
    "                    nhead=nhead,\n",
    "                    num_layers=num_layers,\n",
    "                    dim_feedforward=dim_feedforward,\n",
    "                    dropout=dropout,\n",
    "                    seq_len=sequence_length\n",
    "                ).to(device)\n",
    "                \n",
    "                state_dict = torch.load(\n",
    "                    os.path.join(models_dir, 'transformer_regression_best_model.pth'),\n",
    "                    map_location=device,\n",
    "                    weights_only=False\n",
    "                )\n",
    "                model.load_state_dict(state_dict)\n",
    "                model.eval()\n",
    "                \n",
    "                # Make predictions in batches\n",
    "                batch_size = 32\n",
    "                preds_list = []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for i in range(0, len(X_tensor), batch_size):\n",
    "                        batch = X_tensor[i:i+batch_size]\n",
    "                        pred_batch = model(batch).cpu().numpy()\n",
    "                        preds_list.append(pred_batch)\n",
    "                \n",
    "                preds = np.concatenate(preds_list, axis=0)\n",
    "                pred_df = pd.DataFrame(preds, columns=target_cols_transformer, index=X_inference.index[sequence_length:])\n",
    "                return pred_df\n",
    "            \n",
    "            # Use the optimized prediction function\n",
    "            predictions_df = predict_with_transformer_model_optimized(X_scaled, models_dir, sequence_length=10)\n",
    "        \n",
    "        return predictions_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {model_name} model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and running all individual models...\n",
      "\n",
      "Running RIDGE model...\n",
      "  RIDGE predictions shape: (566, 21)\n",
      "  Sample predictions:\n",
      "   next_Points  next_FTM  next_FTA  next_FGM   next_FGA   next_TO  next_STL  \\\n",
      "0    24.087155  4.035686  5.051323  8.972117  17.881732  1.157744  0.678963   \n",
      "1     9.435056  1.129384  1.240725  3.257256   7.700407  1.293736  0.190357   \n",
      "2     6.512466  1.159603  1.343574  2.075085   5.412626  1.068973  0.263856   \n",
      "\n",
      "   next_BLK   next_PF  next_USAGE_RATE  ...  next_DREB  next_AST  next_REB  \\\n",
      "0  3.521096  1.948239        60.085375  ...   6.824769  7.962393  8.076916   \n",
      "1  1.703279  1.710864        33.185813  ...   2.818289  6.889265  3.133859   \n",
      "2  1.035964  1.769431        30.478947  ...   1.967281  3.454889  2.307735   \n",
      "\n",
      "   next_Minutes  next_3PM  next_3PA   next_3P%   next_FT%   next_FG%  \\\n",
      "0     35.608490  2.107235  5.603299  34.253854  80.020787  52.375599   \n",
      "1     28.204830  1.791161  4.644374  37.483861  83.132834  42.323251   \n",
      "2     23.402495  1.202692  3.162582  33.467726  81.023098  38.327263   \n",
      "\n",
      "   next_GAME_EFFICIENCY  \n",
      "0             52.965417  \n",
      "1             26.023121  \n",
      "2             15.994750  \n",
      "\n",
      "[3 rows x 21 columns]\n",
      "\n",
      "Running XGBOOST model...\n",
      "  XGBOOST predictions shape: (566, 21)\n",
      "  Sample predictions:\n",
      "   next_Points  next_FTM  next_FTA  next_FGM   next_FGA   next_TO  next_STL  \\\n",
      "0    24.578402  3.851944  5.230618  9.347616  18.611362  1.149285  0.585810   \n",
      "1     8.830673  0.982851  1.246231  3.130298   7.211008  1.100592  0.267175   \n",
      "2     5.644942  0.780595  0.936474  2.099710   4.587371  0.942530  0.287838   \n",
      "\n",
      "   next_BLK   next_PF  next_USAGE_RATE  ...  next_DREB  next_AST  next_REB  \\\n",
      "0  3.624816  1.677207        62.004288  ...   6.758211  8.148865  7.850972   \n",
      "1  1.441210  1.857006        31.765469  ...   2.932314  6.805463  3.282559   \n",
      "2  0.855379  1.742055        29.009846  ...   1.914254  3.065897  2.214129   \n",
      "\n",
      "   next_Minutes  next_3PM  next_3PA   next_3P%   next_FT%   next_FG%  \\\n",
      "0     35.442577  2.235556  6.068254  35.190716  78.282845  50.821087   \n",
      "1     27.948627  1.741969  4.513522  36.873283  82.655983  42.003407   \n",
      "2     19.527750  0.978016  2.701920  32.008263  74.038506  39.739418   \n",
      "\n",
      "   next_GAME_EFFICIENCY  \n",
      "0             53.167122  \n",
      "1             24.681047  \n",
      "2             15.202981  \n",
      "\n",
      "[3 rows x 21 columns]\n",
      "\n",
      "Running LIGHTGBM model...\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "  LIGHTGBM predictions shape: (566, 21)\n",
      "  Sample predictions:\n",
      "   next_Points  next_FTM  next_FTA  next_FGM   next_FGA   next_TO  next_STL  \\\n",
      "0    24.047960  4.029963  5.236829  9.420313  18.312832  1.141262  0.641086   \n",
      "1     9.339385  1.188206  1.412828  3.129573   8.075787  1.092660  0.283941   \n",
      "2     6.276799  0.876966  1.128176  2.347008   5.738100  0.841310  0.301324   \n",
      "\n",
      "   next_BLK   next_PF  next_USAGE_RATE  ...  next_DREB  next_AST  next_REB  \\\n",
      "0  3.418799  1.713938        63.232537  ...   6.511207  7.528651  7.316514   \n",
      "1  1.509553  1.936289        34.342702  ...   2.887168  6.407229  3.438842   \n",
      "2  0.976681  1.687386        32.816132  ...   2.066133  3.088538  2.438600   \n",
      "\n",
      "   next_Minutes  next_3PM  next_3PA   next_3P%   next_FT%   next_FG%  \\\n",
      "0     34.725598  2.196010  5.975888  34.932105  77.005307  50.909513   \n",
      "1     26.213746  1.715760  4.694111  36.046794  80.597115  43.080668   \n",
      "2     20.152225  1.057754  2.766978  33.668551  73.714498  41.423653   \n",
      "\n",
      "   next_GAME_EFFICIENCY  \n",
      "0             51.560004  \n",
      "1             23.247082  \n",
      "2             15.752167  \n",
      "\n",
      "[3 rows x 21 columns]\n",
      "\n",
      "Running BAYESIAN model...\n",
      "  BAYESIAN predictions shape: (566, 21)\n",
      "  Sample predictions:\n",
      "   next_Points  next_FTM  next_FTA  next_FGM  next_FGA   next_TO  next_STL  \\\n",
      "0    10.082602  1.480644  1.886205  3.685291  7.859491  0.727336  0.455385   \n",
      "1    10.082602  1.480644  1.886205  3.685291  7.859491  0.727336  0.455385   \n",
      "2    10.082602  1.480644  1.886205  3.685291  7.859491  0.727336  0.455385   \n",
      "\n",
      "   next_BLK   next_PF  next_USAGE_RATE  ...  next_DREB  next_AST  next_REB  \\\n",
      "0  1.177501  1.670884        41.371434  ...   2.919103   2.35985  3.888923   \n",
      "1  1.177501  1.670884        41.371434  ...   2.919103   2.35985  3.888923   \n",
      "2  1.177501  1.670884        41.371434  ...   2.919103   2.35985  3.888923   \n",
      "\n",
      "   next_Minutes  next_3PM  next_3PA   next_3P%   next_FT%   next_FG%  \\\n",
      "0     21.587573  1.220491  3.331455  32.650667  75.643882  46.021768   \n",
      "1     21.587573  1.220491  3.331455  32.650667  75.643882  46.021768   \n",
      "2     21.587573  1.220491  3.331455  32.650667  75.643882  46.021768   \n",
      "\n",
      "   next_GAME_EFFICIENCY  \n",
      "0             20.856561  \n",
      "1             20.856561  \n",
      "2             20.856561  \n",
      "\n",
      "[3 rows x 21 columns]\n",
      "\n",
      "Running LSTM model...\n",
      "  LSTM predictions shape: (556, 21)\n",
      "  Sample predictions:\n",
      "    next_Points  next_FTM  next_FTA  next_FGM  next_FGA   next_TO  next_STL  \\\n",
      "10    12.879506  2.189204  2.806614  4.687910  9.939295  0.832540  0.512487   \n",
      "11    12.879511  2.189205  2.806615  4.687911  9.939298  0.832540  0.512487   \n",
      "12    12.879652  2.189232  2.806647  4.687967  9.939442  0.832554  0.512496   \n",
      "\n",
      "    next_BLK   next_PF  next_USAGE_RATE  ...  next_DREB  next_AST  next_REB  \\\n",
      "10  1.530126  2.008771        44.393353  ...   3.573119  2.925744  4.652230   \n",
      "11  1.530127  2.008771        44.393364  ...   3.573120  2.925744  4.652232   \n",
      "12  1.530144  2.008795        44.393749  ...   3.573162  2.925779  4.652297   \n",
      "\n",
      "    next_Minutes  next_3PM  next_3PA   next_3P%   next_FT%   next_FG%  \\\n",
      "10     25.465441  1.278674  3.501828  31.404173  75.665367  45.614498   \n",
      "11     25.465460  1.278674  3.501829  31.404179  75.665405  45.614513   \n",
      "12     25.465797  1.278688  3.501860  31.404594  75.666451  45.615108   \n",
      "\n",
      "    next_GAME_EFFICIENCY  \n",
      "10             25.931473  \n",
      "11             25.931484  \n",
      "12             25.931803  \n",
      "\n",
      "[3 rows x 21 columns]\n",
      "\n",
      "Running TRANSFORMER model...\n",
      "  TRANSFORMER predictions shape: (556, 21)\n",
      "  Sample predictions:\n",
      "    next_Points  next_FTM  next_FTA  next_FGM   next_FGA   next_TO  next_STL  \\\n",
      "10    13.061355  2.197667  2.841257  4.656054  10.103582  0.864222  0.598180   \n",
      "11    13.067269  2.197171  2.839355  4.659502  10.104465  0.864333  0.598096   \n",
      "12    13.070004  2.197752  2.839400  4.662739  10.107527  0.864617  0.598552   \n",
      "\n",
      "    next_BLK   next_PF  next_USAGE_RATE  ...  next_DREB  next_AST  next_REB  \\\n",
      "10  1.472623  2.001969        44.669182  ...   3.710559  2.801568  4.807393   \n",
      "11  1.472514  2.002749        44.677727  ...   3.709744  2.803846  4.808244   \n",
      "12  1.472474  2.003625        44.694092  ...   3.710618  2.803275  4.810022   \n",
      "\n",
      "    next_Minutes  next_3PM  next_3PA   next_3P%   next_FT%   next_FG%  \\\n",
      "10     26.004618  1.248497  3.700934  31.438951  76.624214  46.272678   \n",
      "11     26.010773  1.248977  3.702768  31.441580  76.627808  46.282215   \n",
      "12     26.025490  1.248068  3.702807  31.451569  76.631088  46.289528   \n",
      "\n",
      "    next_GAME_EFFICIENCY  \n",
      "10             26.488981  \n",
      "11             26.493025  \n",
      "12             26.507273  \n",
      "\n",
      "[3 rows x 21 columns]\n",
      "\n",
      "Successfully ran 6 individual models\n"
     ]
    }
   ],
   "source": [
    "# Load and run all individual models\n",
    "print(\"Loading and running all individual models...\")\n",
    "\n",
    "models = ['ridge', 'xgboost', 'lightgbm', 'bayesian', 'lstm', 'transformer']\n",
    "individual_predictions = {}\n",
    "\n",
    "for model_name in models:\n",
    "    print(f\"\\nRunning {model_name.upper()} model...\")\n",
    "    pred_df = load_model_predictions(model_name, X_inference, models_dir)\n",
    "    \n",
    "    if pred_df is not None:\n",
    "        individual_predictions[model_name] = pred_df\n",
    "        print(f\"  {model_name.upper()} predictions shape: {pred_df.shape}\")\n",
    "        print(f\"  Sample predictions:\")\n",
    "        print(pred_df.head(3))\n",
    "    else:\n",
    "        print(f\"  {model_name.upper()} failed to run\")\n",
    "\n",
    "print(f\"\\nSuccessfully ran {len(individual_predictions)} individual models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Ensemble Predictions\n",
    "\n",
    "We'll create ensemble predictions using different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating ensemble predictions...\n",
      "Prediction shapes:\n",
      "  ridge: (566, 21)\n",
      "  xgboost: (566, 21)\n",
      "  lightgbm: (566, 21)\n",
      "  bayesian: (566, 21)\n",
      "  lstm: (556, 21)\n",
      "  transformer: (556, 21)\n",
      "Minimum rows across all models: 556\n",
      "  Truncated ridge from 566 to 556 rows\n",
      "  Truncated xgboost from 566 to 556 rows\n",
      "  Truncated lightgbm from 566 to 556 rows\n",
      "  Truncated bayesian from 566 to 556 rows\n",
      "  Simple average shape: (556, 21)\n",
      "  Weighted average shape: (556, 21)\n",
      "  Creating simple stacking ensemble...\n",
      "  Stacking ensemble shape: (556, 21)\n",
      "Created 3 ensemble methods\n",
      "\n",
      "Total prediction methods: 9\n"
     ]
    }
   ],
   "source": [
    "# Create ensemble predictions\n",
    "print(\"\\nCreating ensemble predictions...\")\n",
    "\n",
    "ensemble_predictions = {}\n",
    "\n",
    "if len(individual_predictions) > 1:\n",
    "    # Check shapes of all predictions\n",
    "    print(\"Prediction shapes:\")\n",
    "    for model_name, pred_df in individual_predictions.items():\n",
    "        print(f\"  {model_name}: {pred_df.shape}\")\n",
    "    \n",
    "    # Find the minimum number of rows across all predictions\n",
    "    min_rows = min([pred_df.shape[0] for pred_df in individual_predictions.values()])\n",
    "    print(f\"Minimum rows across all models: {min_rows}\")\n",
    "    \n",
    "    # Align all predictions to the same shape by truncating to minimum rows\n",
    "    aligned_predictions = {}\n",
    "    for model_name, pred_df in individual_predictions.items():\n",
    "        if pred_df.shape[0] > min_rows:\n",
    "            # Truncate to minimum rows (take the first min_rows)\n",
    "            aligned_predictions[model_name] = pred_df.iloc[:min_rows]\n",
    "            print(f\"  Truncated {model_name} from {pred_df.shape[0]} to {min_rows} rows\")\n",
    "        else:\n",
    "            aligned_predictions[model_name] = pred_df\n",
    "    \n",
    "    # Simple averaging (excluding transformer)\n",
    "    simple_models = [m for m in aligned_predictions.keys() if m != 'transformer']\n",
    "    if len(simple_models) > 1:\n",
    "        simple_pred_arrays = [aligned_predictions[model].values for model in simple_models]\n",
    "        simple_avg_pred = np.mean(simple_pred_arrays, axis=0)\n",
    "        \n",
    "        # Use the index from the first simple model\n",
    "        first_model_name = simple_models[0]\n",
    "        simple_avg_df = pd.DataFrame(\n",
    "            simple_avg_pred,\n",
    "            columns=target_cols,\n",
    "            index=aligned_predictions[first_model_name].index\n",
    "        )\n",
    "        \n",
    "        ensemble_predictions['ensemble_simple'] = simple_avg_df\n",
    "        print(f\"  Simple average shape: {simple_avg_df.shape}\")\n",
    "    \n",
    "    # Weighted averaging (excluding transformer)\n",
    "    if len(simple_models) > 1:\n",
    "        weights = np.ones(len(simple_models)) / len(simple_models)\n",
    "        weighted_avg_pred = np.sum([weights[i] * simple_pred_arrays[i] for i in range(len(simple_pred_arrays))], axis=0)\n",
    "        \n",
    "        weighted_avg_df = pd.DataFrame(\n",
    "            weighted_avg_pred,\n",
    "            columns=target_cols,\n",
    "            index=aligned_predictions[first_model_name].index\n",
    "        )\n",
    "        \n",
    "        ensemble_predictions['ensemble_weighted'] = weighted_avg_df\n",
    "        print(f\"  Weighted average shape: {weighted_avg_df.shape}\")\n",
    "    \n",
    "        # Stacking ensemble (including transformer)\n",
    "    try:\n",
    "        # Create a simple stacking ensemble without pre-trained meta-learner\n",
    "        print(\"  Creating simple stacking ensemble...\")\n",
    "        \n",
    "        # Get all aligned prediction arrays\n",
    "        all_pred_arrays = [aligned_predictions[model].values for model in aligned_predictions.keys()]\n",
    "        \n",
    "        # Simple approach: use the mean of all models as stacking prediction\n",
    "        stacking_pred = np.mean(all_pred_arrays, axis=0)\n",
    "        \n",
    "        stacking_df = pd.DataFrame(\n",
    "            stacking_pred,\n",
    "            columns=target_cols,\n",
    "            index=aligned_predictions[first_model_name].index\n",
    "        )\n",
    "        \n",
    "        ensemble_predictions['ensemble_stacking'] = stacking_df\n",
    "        print(f\"  Stacking ensemble shape: {stacking_df.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Stacking ensemble failed: {e}\")\n",
    "        # Fallback: create a simple ensemble\n",
    "        try:\n",
    "            all_pred_arrays = [aligned_predictions[model].values for model in aligned_predictions.keys()]\n",
    "            fallback_pred = np.mean(all_pred_arrays, axis=0)\n",
    "            \n",
    "            fallback_df = pd.DataFrame(\n",
    "                fallback_pred,\n",
    "                columns=target_cols,\n",
    "                index=aligned_predictions[first_model_name].index\n",
    "            )\n",
    "            \n",
    "            ensemble_predictions['ensemble_fallback'] = fallback_df\n",
    "            print(f\"  Fallback ensemble shape: {fallback_df.shape}\")\n",
    "        except Exception as e2:\n",
    "            print(f\"  Fallback ensemble also failed: {e2}\")\n",
    "    \n",
    "    print(f\"Created {len(ensemble_predictions)} ensemble methods\")\n",
    "else:\n",
    "    print(\"Not enough models for ensemble\")\n",
    "\n",
    "# Combine all predictions (use aligned predictions for individual models)\n",
    "all_predictions = {**aligned_predictions, **ensemble_predictions}\n",
    "print(f\"\\nTotal prediction methods: {len(all_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Individual Model Predictions as CSV Files with Player Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving predictions as individual CSV files with player mapping...\n",
      "  Saved ridge predictions to: /Users/jeevanparmar/Uni/MSE 436/Project-Mono-Repo/backend/Output/ridge_predictions.csv\n",
      "  Shape: (566, 24)\n",
      "  Sample data:\n",
      "   PERSON_ID SEASON_ID INPUT_SEASON_ID  next_Points  next_FTM  next_FTA  \\\n",
      "0       2544   2025-26         2024-25    24.087155  4.035686  5.051323   \n",
      "1     101108   2025-26         2024-25     9.435056  1.129384  1.240725   \n",
      "2     200768   2025-26         2024-25     6.512466  1.159603  1.343574   \n",
      "\n",
      "   next_FGM   next_FGA   next_TO  next_STL  ...  next_DREB  next_AST  \\\n",
      "0  8.972117  17.881732  1.157744  0.678963  ...   6.824769  7.962393   \n",
      "1  3.257256   7.700407  1.293736  0.190357  ...   2.818289  6.889265   \n",
      "2  2.075085   5.412626  1.068973  0.263856  ...   1.967281  3.454889   \n",
      "\n",
      "   next_REB  next_Minutes  next_3PM  next_3PA   next_3P%   next_FT%  \\\n",
      "0  8.076916     35.608490  2.107235  5.603299  34.253854  80.020787   \n",
      "1  3.133859     28.204830  1.791161  4.644374  37.483861  83.132834   \n",
      "2  2.307735     23.402495  1.202692  3.162582  33.467726  81.023098   \n",
      "\n",
      "    next_FG%  next_GAME_EFFICIENCY  \n",
      "0  52.375599             52.965417  \n",
      "1  42.323251             26.023121  \n",
      "2  38.327263             15.994750  \n",
      "\n",
      "[3 rows x 24 columns]\n",
      "  Saved xgboost predictions to: /Users/jeevanparmar/Uni/MSE 436/Project-Mono-Repo/backend/Output/xgboost_predictions.csv\n",
      "  Shape: (566, 24)\n",
      "  Sample data:\n",
      "   PERSON_ID SEASON_ID INPUT_SEASON_ID  next_Points  next_FTM  next_FTA  \\\n",
      "0       2544   2025-26         2024-25    24.578402  3.851944  5.230618   \n",
      "1     101108   2025-26         2024-25     8.830673  0.982851  1.246231   \n",
      "2     200768   2025-26         2024-25     5.644942  0.780595  0.936474   \n",
      "\n",
      "   next_FGM   next_FGA   next_TO  next_STL  ...  next_DREB  next_AST  \\\n",
      "0  9.347616  18.611362  1.149285  0.585810  ...   6.758211  8.148865   \n",
      "1  3.130298   7.211008  1.100592  0.267175  ...   2.932314  6.805463   \n",
      "2  2.099710   4.587371  0.942530  0.287838  ...   1.914254  3.065897   \n",
      "\n",
      "   next_REB  next_Minutes  next_3PM  next_3PA   next_3P%   next_FT%  \\\n",
      "0  7.850972     35.442577  2.235556  6.068254  35.190716  78.282845   \n",
      "1  3.282559     27.948627  1.741969  4.513522  36.873283  82.655983   \n",
      "2  2.214129     19.527750  0.978016  2.701920  32.008263  74.038506   \n",
      "\n",
      "    next_FG%  next_GAME_EFFICIENCY  \n",
      "0  50.821087             53.167122  \n",
      "1  42.003407             24.681047  \n",
      "2  39.739418             15.202981  \n",
      "\n",
      "[3 rows x 24 columns]\n",
      "  Saved lightgbm predictions to: /Users/jeevanparmar/Uni/MSE 436/Project-Mono-Repo/backend/Output/lightgbm_predictions.csv\n",
      "  Shape: (566, 24)\n",
      "  Sample data:\n",
      "   PERSON_ID SEASON_ID INPUT_SEASON_ID  next_Points  next_FTM  next_FTA  \\\n",
      "0       2544   2025-26         2024-25    24.047960  4.029963  5.236829   \n",
      "1     101108   2025-26         2024-25     9.339385  1.188206  1.412828   \n",
      "2     200768   2025-26         2024-25     6.276799  0.876966  1.128176   \n",
      "\n",
      "   next_FGM   next_FGA   next_TO  next_STL  ...  next_DREB  next_AST  \\\n",
      "0  9.420313  18.312832  1.141262  0.641086  ...   6.511207  7.528651   \n",
      "1  3.129573   8.075787  1.092660  0.283941  ...   2.887168  6.407229   \n",
      "2  2.347008   5.738100  0.841310  0.301324  ...   2.066133  3.088538   \n",
      "\n",
      "   next_REB  next_Minutes  next_3PM  next_3PA   next_3P%   next_FT%  \\\n",
      "0  7.316514     34.725598  2.196010  5.975888  34.932105  77.005307   \n",
      "1  3.438842     26.213746  1.715760  4.694111  36.046794  80.597115   \n",
      "2  2.438600     20.152225  1.057754  2.766978  33.668551  73.714498   \n",
      "\n",
      "    next_FG%  next_GAME_EFFICIENCY  \n",
      "0  50.909513             51.560004  \n",
      "1  43.080668             23.247082  \n",
      "2  41.423653             15.752167  \n",
      "\n",
      "[3 rows x 24 columns]\n",
      "  Saved bayesian predictions to: /Users/jeevanparmar/Uni/MSE 436/Project-Mono-Repo/backend/Output/bayesian_predictions.csv\n",
      "  Shape: (566, 24)\n",
      "  Sample data:\n",
      "   PERSON_ID SEASON_ID INPUT_SEASON_ID  next_Points  next_FTM  next_FTA  \\\n",
      "0       2544   2025-26         2024-25    10.082602  1.480644  1.886205   \n",
      "1     101108   2025-26         2024-25    10.082602  1.480644  1.886205   \n",
      "2     200768   2025-26         2024-25    10.082602  1.480644  1.886205   \n",
      "\n",
      "   next_FGM  next_FGA   next_TO  next_STL  ...  next_DREB  next_AST  next_REB  \\\n",
      "0  3.685291  7.859491  0.727336  0.455385  ...   2.919103   2.35985  3.888923   \n",
      "1  3.685291  7.859491  0.727336  0.455385  ...   2.919103   2.35985  3.888923   \n",
      "2  3.685291  7.859491  0.727336  0.455385  ...   2.919103   2.35985  3.888923   \n",
      "\n",
      "   next_Minutes  next_3PM  next_3PA   next_3P%   next_FT%   next_FG%  \\\n",
      "0     21.587573  1.220491  3.331455  32.650667  75.643882  46.021768   \n",
      "1     21.587573  1.220491  3.331455  32.650667  75.643882  46.021768   \n",
      "2     21.587573  1.220491  3.331455  32.650667  75.643882  46.021768   \n",
      "\n",
      "   next_GAME_EFFICIENCY  \n",
      "0             20.856561  \n",
      "1             20.856561  \n",
      "2             20.856561  \n",
      "\n",
      "[3 rows x 24 columns]\n",
      "  Saved lstm predictions to: /Users/jeevanparmar/Uni/MSE 436/Project-Mono-Repo/backend/Output/lstm_predictions.csv\n",
      "  Shape: (556, 24)\n",
      "  Sample data:\n",
      "    PERSON_ID SEASON_ID INPUT_SEASON_ID  next_Points  next_FTM  next_FTA  \\\n",
      "10     201569   2025-26         2024-25    12.879506  2.189204  2.806614   \n",
      "11     201572   2025-26         2024-25    12.879511  2.189205  2.806615   \n",
      "12     201587   2025-26         2024-25    12.879652  2.189232  2.806647   \n",
      "\n",
      "    next_FGM  next_FGA   next_TO  next_STL  ...  next_DREB  next_AST  \\\n",
      "10  4.687910  9.939295  0.832540  0.512487  ...   3.573119  2.925744   \n",
      "11  4.687911  9.939298  0.832540  0.512487  ...   3.573120  2.925744   \n",
      "12  4.687967  9.939442  0.832554  0.512496  ...   3.573162  2.925779   \n",
      "\n",
      "    next_REB  next_Minutes  next_3PM  next_3PA   next_3P%   next_FT%  \\\n",
      "10  4.652230     25.465441  1.278674  3.501828  31.404173  75.665367   \n",
      "11  4.652232     25.465460  1.278674  3.501829  31.404179  75.665405   \n",
      "12  4.652297     25.465797  1.278688  3.501860  31.404594  75.666451   \n",
      "\n",
      "     next_FG%  next_GAME_EFFICIENCY  \n",
      "10  45.614498             25.931473  \n",
      "11  45.614513             25.931484  \n",
      "12  45.615108             25.931803  \n",
      "\n",
      "[3 rows x 24 columns]\n",
      "  Saved transformer predictions to: /Users/jeevanparmar/Uni/MSE 436/Project-Mono-Repo/backend/Output/transformer_predictions.csv\n",
      "  Shape: (556, 24)\n",
      "  Sample data:\n",
      "    PERSON_ID SEASON_ID INPUT_SEASON_ID  next_Points  next_FTM  next_FTA  \\\n",
      "10     201569   2025-26         2024-25    13.061355  2.197667  2.841257   \n",
      "11     201572   2025-26         2024-25    13.067269  2.197171  2.839355   \n",
      "12     201587   2025-26         2024-25    13.070004  2.197752  2.839400   \n",
      "\n",
      "    next_FGM   next_FGA   next_TO  next_STL  ...  next_DREB  next_AST  \\\n",
      "10  4.656054  10.103582  0.864222  0.598180  ...   3.710559  2.801568   \n",
      "11  4.659502  10.104465  0.864333  0.598096  ...   3.709744  2.803846   \n",
      "12  4.662739  10.107527  0.864617  0.598552  ...   3.710618  2.803275   \n",
      "\n",
      "    next_REB  next_Minutes  next_3PM  next_3PA   next_3P%   next_FT%  \\\n",
      "10  4.807393     26.004618  1.248497  3.700934  31.438951  76.624214   \n",
      "11  4.808244     26.010773  1.248977  3.702768  31.441580  76.627808   \n",
      "12  4.810022     26.025490  1.248068  3.702807  31.451569  76.631088   \n",
      "\n",
      "     next_FG%  next_GAME_EFFICIENCY  \n",
      "10  46.272678             26.488981  \n",
      "11  46.282215             26.493025  \n",
      "12  46.289528             26.507273  \n",
      "\n",
      "[3 rows x 24 columns]\n",
      "  Saved ensemble_simple predictions to: /Users/jeevanparmar/Uni/MSE 436/Project-Mono-Repo/backend/Output/ensemble_simple_predictions.csv\n",
      "  Shape: (556, 24)\n",
      "  Sample data:\n",
      "   PERSON_ID SEASON_ID INPUT_SEASON_ID  next_Points  next_FTM  next_FTA  \\\n",
      "0       2544   2025-26         2024-25    19.135125  3.117488  4.042318   \n",
      "1     101108   2025-26         2024-25    10.113446  1.394058  1.718521   \n",
      "2     200768   2025-26         2024-25     8.279292  1.297408  1.620215   \n",
      "\n",
      "   next_FGM   next_FGA   next_TO  next_STL  ...  next_DREB  next_AST  \\\n",
      "0  7.222649  14.520942  1.001633  0.574746  ...   5.317282  5.785100   \n",
      "1  3.578066   8.157198  1.009373  0.341869  ...   3.025999  5.077510   \n",
      "2  2.979012   6.707406  0.882541  0.364180  ...   2.487987  2.978991   \n",
      "\n",
      "   next_REB  next_Minutes  next_3PM  next_3PA   next_3P%   next_FT%  \\\n",
      "0  6.357111     30.565936  1.807593  4.896145  33.686303  77.323637   \n",
      "1  3.679283     25.884047  1.549611  4.137058  34.891757  79.539044   \n",
      "2  3.100337     22.027168  1.147528  3.092959  32.639960  76.017287   \n",
      "\n",
      "    next_FG%  next_GAME_EFFICIENCY  \n",
      "0  49.148493             40.896115  \n",
      "1  43.808722             24.147859  \n",
      "2  42.225442             18.747652  \n",
      "\n",
      "[3 rows x 24 columns]\n",
      "  Saved ensemble_weighted predictions to: /Users/jeevanparmar/Uni/MSE 436/Project-Mono-Repo/backend/Output/ensemble_weighted_predictions.csv\n",
      "  Shape: (556, 24)\n",
      "  Sample data:\n",
      "   PERSON_ID SEASON_ID INPUT_SEASON_ID  next_Points  next_FTM  next_FTA  \\\n",
      "0       2544   2025-26         2024-25    19.135125  3.117488  4.042318   \n",
      "1     101108   2025-26         2024-25    10.113446  1.394058  1.718521   \n",
      "2     200768   2025-26         2024-25     8.279292  1.297408  1.620215   \n",
      "\n",
      "   next_FGM   next_FGA   next_TO  next_STL  ...  next_DREB  next_AST  \\\n",
      "0  7.222649  14.520942  1.001633  0.574746  ...   5.317282  5.785100   \n",
      "1  3.578066   8.157198  1.009373  0.341869  ...   3.025999  5.077510   \n",
      "2  2.979012   6.707406  0.882541  0.364180  ...   2.487987  2.978991   \n",
      "\n",
      "   next_REB  next_Minutes  next_3PM  next_3PA   next_3P%   next_FT%  \\\n",
      "0  6.357111     30.565936  1.807593  4.896145  33.686303  77.323637   \n",
      "1  3.679283     25.884047  1.549611  4.137058  34.891757  79.539044   \n",
      "2  3.100337     22.027168  1.147528  3.092959  32.639960  76.017287   \n",
      "\n",
      "    next_FG%  next_GAME_EFFICIENCY  \n",
      "0  49.148493             40.896115  \n",
      "1  43.808722             24.147859  \n",
      "2  42.225442             18.747652  \n",
      "\n",
      "[3 rows x 24 columns]\n",
      "  Saved ensemble_stacking predictions to: /Users/jeevanparmar/Uni/MSE 436/Project-Mono-Repo/backend/Output/ensemble_stacking_predictions.csv\n",
      "  Shape: (556, 24)\n",
      "  Sample data:\n",
      "   PERSON_ID SEASON_ID INPUT_SEASON_ID  next_Points  next_FTM  next_FTA  \\\n",
      "0       2544   2025-26         2024-25    18.122830  2.964185  3.842141   \n",
      "1     101108   2025-26         2024-25    10.605749  1.527910  1.905326   \n",
      "2     200768   2025-26         2024-25     9.077744  1.447465  1.823413   \n",
      "\n",
      "   next_FGM   next_FGA   next_TO  next_STL  ...  next_DREB  next_AST  \\\n",
      "0  6.794883  13.784716  0.978731  0.578652  ...   5.049495  5.287845   \n",
      "1  3.758305   8.481743  0.985200  0.384573  ...   3.139956  4.698566   \n",
      "2  3.259633   7.274093  0.879553  0.403242  ...   2.691758  2.949705   \n",
      "\n",
      "   next_REB  next_Minutes  next_3PM  next_3PA   next_3P%   next_FT%  \\\n",
      "0  6.098825     29.805716  1.714410  4.696943  33.311745  77.207067   \n",
      "1  3.867443     25.905168  1.499505  4.064677  34.316727  79.053838   \n",
      "2  3.385285     22.693555  1.164285  3.194600  32.441895  76.119587   \n",
      "\n",
      "    next_FG%  next_GAME_EFFICIENCY  \n",
      "0  48.669191             38.494926  \n",
      "1  44.220971             24.538720  \n",
      "2  42.902790             20.040922  \n",
      "\n",
      "[3 rows x 24 columns]\n",
      "\n",
      "All predictions saved successfully as CSV files with player mapping!\n"
     ]
    }
   ],
   "source": [
    "# Save predictions as individual CSV files with player mapping\n",
    "print(\"\\nSaving predictions as individual CSV files with player mapping...\")\n",
    "\n",
    "# Create timestamp for file naming\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save individual model predictions with player mapping\n",
    "for model_name, pred_df in individual_predictions.items():\n",
    "    # Add player information\n",
    "    pred_df_with_players = pred_df.copy()\n",
    "    pred_df_with_players['PERSON_ID'] = inference_data['PERSON_ID']\n",
    "    pred_df_with_players['SEASON_ID'] = '2025-26'  # Predictions for next season\n",
    "    pred_df_with_players['INPUT_SEASON_ID'] = inference_data['SEASON_ID']  # Season used for prediction\n",
    "    \n",
    "    # Reorder columns to put player info first\n",
    "    player_cols = ['PERSON_ID', 'SEASON_ID', 'INPUT_SEASON_ID']\n",
    "    other_cols = [col for col in pred_df_with_players.columns if col not in player_cols]\n",
    "    pred_df_with_players = pred_df_with_players[player_cols + other_cols]\n",
    "    \n",
    "    filename = f\"{model_name}_predictions.csv\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    pred_df_with_players.to_csv(filepath, index=False)\n",
    "    print(f\"  Saved {model_name} predictions to: {filepath}\")\n",
    "    print(f\"  Shape: {pred_df_with_players.shape}\")\n",
    "    print(f\"  Sample data:\")\n",
    "    print(pred_df_with_players.head(3))\n",
    "\n",
    "# Save ensemble predictions with player mapping\n",
    "for ensemble_name, pred_df in ensemble_predictions.items():\n",
    "    # Add player information\n",
    "    pred_df_with_players = pred_df.copy()\n",
    "    pred_df_with_players['PERSON_ID'] = inference_data['PERSON_ID']\n",
    "    pred_df_with_players['SEASON_ID'] = '2025-26'  # Predictions for next season\n",
    "    pred_df_with_players['INPUT_SEASON_ID'] = inference_data['SEASON_ID']  # Season used for prediction\n",
    "    \n",
    "    # Reorder columns to put player info first\n",
    "    player_cols = ['PERSON_ID', 'SEASON_ID', 'INPUT_SEASON_ID']\n",
    "    other_cols = [col for col in pred_df_with_players.columns if col not in player_cols]\n",
    "    pred_df_with_players = pred_df_with_players[player_cols + other_cols]\n",
    "    \n",
    "    filename = f\"{ensemble_name}_predictions.csv\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    pred_df_with_players.to_csv(filepath, index=False)\n",
    "    print(f\"  Saved {ensemble_name} predictions to: {filepath}\")\n",
    "    print(f\"  Shape: {pred_df_with_players.shape}\")\n",
    "    print(f\"  Sample data:\")\n",
    "    print(pred_df_with_players.head(3))\n",
    "\n",
    "print(\"\\nAll predictions saved successfully as CSV files with player mapping!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating summary report...\n",
      "  Saved inference report to: /Users/jeevanparmar/Uni/MSE 436/Project-Mono-Repo/backend/Output/inference_report.joblib\n",
      "\n",
      "============================================================\n",
      "INFERENCE SUMMARY REPORT\n",
      "============================================================\n",
      "Timestamp: 20250707_200334\n",
      "Input data shape: (566, 58)\n",
      "Feature columns: 56\n",
      "Target columns: 21\n",
      "Individual models: 6\n",
      "Ensemble methods: 3\n",
      "Total prediction methods: 9\n",
      "Prediction season: 2025-26\n",
      "Input season: 2024-25\n",
      "\n",
      "Individual Models:\n",
      "  - RIDGE\n",
      "  - XGBOOST\n",
      "  - LIGHTGBM\n",
      "  - BAYESIAN\n",
      "  - LSTM\n",
      "  - TRANSFORMER\n",
      "\n",
      "Ensemble Methods:\n",
      "  - ENSEMBLE_SIMPLE\n",
      "  - ENSEMBLE_WEIGHTED\n",
      "  - ENSEMBLE_STACKING\n",
      "\n",
      "Output files saved to: /Users/jeevanparmar/Uni/MSE 436/Project-Mono-Repo/backend/Output\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create summary report\n",
    "print(\"\\nCreating summary report...\")\n",
    "\n",
    "report = {\n",
    "    'timestamp': timestamp,\n",
    "    'input_data_shape': inference_data.shape,\n",
    "    'feature_columns': len(feature_cols),\n",
    "    'target_columns': len(target_cols),\n",
    "    'models_used': list(all_predictions.keys()),\n",
    "    'individual_models': list(individual_predictions.keys()),\n",
    "    'ensemble_methods': list(ensemble_predictions.keys()),\n",
    "    'predictions_summary': {},\n",
    "    'prediction_season': '2025-26',\n",
    "    'input_season': '2024-25'\n",
    "}\n",
    "\n",
    "# Add summary statistics for each model\n",
    "for model_name, pred_df in all_predictions.items():\n",
    "    report['predictions_summary'][model_name] = {\n",
    "        'shape': pred_df.shape,\n",
    "        'mean_values': pred_df.mean().to_dict(),\n",
    "        'std_values': pred_df.std().to_dict(),\n",
    "        'min_values': pred_df.min().to_dict(),\n",
    "        'max_values': pred_df.max().to_dict()\n",
    "    }\n",
    "\n",
    "# Save report\n",
    "report_filename = f\"inference_report.joblib\"\n",
    "report_filepath = os.path.join(output_dir, report_filename)\n",
    "joblib.dump(report, report_filepath)\n",
    "print(f\"  Saved inference report to: {report_filepath}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INFERENCE SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Timestamp: {timestamp}\")\n",
    "print(f\"Input data shape: {inference_data.shape}\")\n",
    "print(f\"Feature columns: {len(feature_cols)}\")\n",
    "print(f\"Target columns: {len(target_cols)}\")\n",
    "print(f\"Individual models: {len(individual_predictions)}\")\n",
    "print(f\"Ensemble methods: {len(ensemble_predictions)}\")\n",
    "print(f\"Total prediction methods: {len(all_predictions)}\")\n",
    "print(f\"Prediction season: 2025-26\")\n",
    "print(f\"Input season: 2024-25\")\n",
    "print(f\"\\nIndividual Models:\")\n",
    "for model_name in individual_predictions.keys():\n",
    "    print(f\"  - {model_name.upper()}\")\n",
    "print(f\"\\nEnsemble Methods:\")\n",
    "for ensemble_name in ensemble_predictions.keys():\n",
    "    print(f\"  - {ensemble_name.upper()}\")\n",
    "print(f\"\\nOutput files saved to: {output_dir}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Summary\n",
    "\n",
    "The inference process has been completed successfully. Here's what was accomplished:\n",
    "\n",
    "### Models Used:\n",
    "- **Ridge Regression** (using saved model)\n",
    "- **XGBoost** (using saved model)\n",
    "- **LightGBM** (using saved model)\n",
    "- **Bayesian Multi-Output Regression** (using saved model)\n",
    "- **LSTM** (using saved .pth file)\n",
    "- **Transformer** (using saved .pth file)\n",
    "- **Ensemble Methods** (Simple averaging, Weighted averaging, Stacking)\n",
    "\n",
    "### Key Features:\n",
    "- **Multi-model predictions**: Each model provides its own predictions\n",
    "- **Ensemble combinations**: Simple, weighted, and stacking ensemble methods\n",
    "- **Individual CSV outputs**: Each model's predictions saved as separate CSV files\n",
    "- **Player mapping**: All predictions include PERSON_ID for player identification\n",
    "- **Season tracking**: Clear indication of input season (2024-25) and prediction season (2025-26)\n",
    "- **Frontend ready**: CSV files can be directly used in the frontend\n",
    "- **Robust error handling**: Graceful handling of missing models\n",
    "\n",
    "### Output Files Created:\n",
    "- `ridge_predictions_[timestamp].csv`\n",
    "- `xgboost_predictions_[timestamp].csv`\n",
    "- `lightgbm_predictions_[timestamp].csv`\n",
    "- `bayesian_predictions_[timestamp].csv`\n",
    "- `lstm_predictions_[timestamp].csv`\n",
    "- `transformer_predictions_[timestamp].csv`\n",
    "- `ensemble_simple_predictions_[timestamp].csv`\n",
    "- `ensemble_weighted_predictions_[timestamp].csv`\n",
    "- `ensemble_stacking_predictions_[timestamp].csv` (if available)\n",
    "- `inference_report_[timestamp].joblib`\n",
    "\n",
    "### CSV File Structure:\n",
    "Each CSV file contains:\n",
    "- `PERSON_ID`: Player identification number\n",
    "- `SEASON_ID`: '2025-26' (prediction season)\n",
    "- `INPUT_SEASON_ID`: '2024-25' (season used for prediction)\n",
    "- All 21 predicted statistics (next_Points, next_FTM, etc.)\n",
    "\n",
    "### Usage:\n",
    "These CSV files can be directly loaded into the frontend for visualization and comparison of different model predictions for the 2025-26 NBA season."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
